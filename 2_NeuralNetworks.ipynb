{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks Laboratory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Neural Network should always be built considering the dataset at hand.\n",
    "\n",
    "In our case, we will continue our example with the Wine Dataset we built in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenericDataset(Dataset):\n",
    "    def __init__(self, targets_file, data_file, transform=None, target_transform=None):\n",
    "        self.targets_file = pd.read_csv(targets_file)\n",
    "        self.data_dir = pd.read_csv(data_file)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.targets_file)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data_dir.iloc[idx].to_numpy(dtype=np.float32), self.targets_file.iloc[idx].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = os.path.join('data', 'iris')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "WineDataset = GenericDataset(targets_file=os.path.join(folder, 'targets.csv'), data_file=os.path.join(folder, 'data.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.9 * len(WineDataset))\n",
    "test_size = len(WineDataset) - train_size\n",
    "train_dataset, test_dataset = torch.utils.data.random_split(WineDataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.6722, -0.5858,  1.0436,  1.3121],\n",
       "         [-1.2600, -0.1245, -1.3368, -1.1776],\n",
       "         [ 1.5176, -0.1245,  1.2136,  1.1811],\n",
       "         [ 0.5515, -1.7390,  0.3635,  0.1328],\n",
       "         [ 0.6722, -0.3552,  0.3068,  0.1328],\n",
       "         [-1.3807,  0.3367, -1.2234, -1.3086],\n",
       "         [ 0.1892, -0.3552,  0.4202,  0.3948],\n",
       "         [-0.8977,  1.7205, -1.2234, -1.3086],\n",
       "         [ 0.3100, -0.3552,  0.5335,  0.2638],\n",
       "         [ 0.4307,  0.7980,  0.9302,  1.4431],\n",
       "         [-1.6223, -1.7390, -1.3935, -1.1776],\n",
       "         [-1.5015,  0.3367, -1.3368, -1.3086],\n",
       "         [-0.0523, -0.8164,  0.7602,  0.9190],\n",
       "         [-0.1731, -0.5858,  0.4202,  0.1328],\n",
       "         [-1.1392, -1.2777,  0.4202,  0.6569],\n",
       "         [-1.0184, -2.4308, -0.1466, -0.2603],\n",
       "         [ 1.2761,  0.1061,  0.7602,  1.4431],\n",
       "         [-1.0184,  1.2592, -1.3368, -1.3086],\n",
       "         [ 2.4837,  1.7205,  1.4970,  1.0500],\n",
       "         [ 0.4307, -0.5858,  0.5902,  0.7880],\n",
       "         [ 0.6722,  0.1061,  0.9869,  0.7880],\n",
       "         [ 1.0345,  0.5674,  1.1003,  1.7052],\n",
       "         [-0.0523, -0.8164,  0.7602,  0.9190],\n",
       "         [-1.3807,  0.3367, -1.3935, -1.3086],\n",
       "         [ 1.1553,  0.3367,  1.2136,  1.4431],\n",
       "         [-0.2939, -0.1245,  0.1935,  0.1328],\n",
       "         [-1.2600,  0.7980, -1.2234, -1.3086],\n",
       "         [ 1.7591, -0.3552,  1.4403,  0.7880],\n",
       "         [ 0.5515, -1.2777,  0.6469,  0.3948],\n",
       "         [ 2.2422,  1.7205,  1.6670,  1.3121],\n",
       "         [ 1.2761,  0.1061,  0.9302,  1.1811],\n",
       "         [ 0.1892, -1.9696,  0.1368, -0.2603]]),\n",
       " tensor([2, 0, 2, 1, 1, 0, 1, 0, 1, 2, 0, 0, 2, 1, 2, 1, 2, 0, 2, 2, 2, 2, 2, 0,\n",
       "         2, 1, 0, 2, 1, 2, 2, 1])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch is useful to build fully customizable Neural Networks.\n",
    "Keep in mind these three things:\n",
    "\n",
    "1. Our Neural Network will extend the class ```nn.Module```. \n",
    "\n",
    "2. The layers will be initialized inside ```__init__```.\n",
    "\n",
    "3. The operations on the input data are defined in the ```forward``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FirstNeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(4, 10)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(10, 40)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(40, 20)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.output_layer = nn.Linear(20, 3)  \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FirstNeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "  \n",
    "  size = len(dataloader.dataset)\n",
    "  for batch, (X, y) in enumerate(dataloader):\n",
    "\n",
    "    pred = model(X)\n",
    "    loss = loss_fn(pred, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if batch % 100 == 0:\n",
    "      loss, current = loss.item(), (batch + 1) * len(X)\n",
    "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "  size = len(dataloader.dataset)\n",
    "  num_batches = len(dataloader)\n",
    "  test_loss, correct = 0, 0\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for X, y in dataloader:\n",
    "      pred = model(X)\n",
    "      test_loss += loss_fn(pred, y).item()\n",
    "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "  test_loss /= num_batches\n",
    "  correct /= size\n",
    "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-1\n",
    "batch_size = 32\n",
    "epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.091637  [   32/  135]\n",
      "Test Error: \n",
      " Accuracy: 26.7%, Avg loss: 1.150203 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.090529  [   32/  135]\n",
      "Test Error: \n",
      " Accuracy: 26.7%, Avg loss: 1.146206 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.077711  [   32/  135]\n",
      "Test Error: \n",
      " Accuracy: 26.7%, Avg loss: 1.127584 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.047767  [   32/  135]\n",
      "Test Error: \n",
      " Accuracy: 26.7%, Avg loss: 1.102759 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.055685  [   32/  135]\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 1.068471 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.034009  [   32/  135]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Accuracy: 33.3%, Avg loss: 1.039135 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.012503  [   32/  135]\n",
      "Test Error: \n",
      " Accuracy: 40.0%, Avg loss: 1.008058 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.989487  [   32/  135]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.926916 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.938347  [   32/  135]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.840304 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.860558  [   32/  135]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.746502 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.801235  [   32/  135]\n",
      "Test Error: \n",
      " Accuracy: 73.3%, Avg loss: 0.641173 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.673876  [   32/  135]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.542615 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.654589  [   32/  135]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.475310 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.633316  [   32/  135]\n",
      "Test Error: \n",
      " Accuracy: 80.0%, Avg loss: 0.411187 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.460960  [   32/  135]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.374699 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.421677  [   32/  135]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.330686 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.416861  [   32/  135]\n",
      "Test Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.305861 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.292792  [   32/  135]\n",
      "Test Error: \n",
      " Accuracy: 86.7%, Avg loss: 0.296022 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.363184  [   32/  135]\n",
      "Test Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.256473 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.405194  [   32/  135]\n",
      "Test Error: \n",
      " Accuracy: 93.3%, Avg loss: 0.242138 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ir-experiment",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
