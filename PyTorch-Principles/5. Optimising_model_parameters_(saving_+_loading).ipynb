{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "jvAXdKdjpO2B"
      },
      "source": [
        "# Optimising Model Parameters\n",
        "\n",
        "Now that we have a model and data it's time to train, validate and test our model by optimizing its parameters on our data. Training a model is an iterative process; in each iteration the model makes a guess about the output, calculates the error in its guess (*loss*), collects the derivatives of the error with respect to its parameters, and optimizes these parameters using gradient descent. For a more detailed walkthrough of this process, check out this video on [backpropagation from 3Blue1Brown](https://www.youtube.com/watch?v=tIeHLnjs5U8)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T1qvnfMektd7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "3GIXf3rnpb-0"
      },
      "source": [
        "## Prerequisite Code\n",
        "\n",
        "We load the code from the previous lessons on **Datasets and DataLoaders** and **Build the Neural Network**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZfA6r-1XpmjX"
      },
      "outputs": [],
      "source": [
        "training_data = datasets.FashionMNIST(\n",
        "  root=\"data\",\n",
        "  train=True,\n",
        "  download=True,\n",
        "  transform = Compose([ \n",
        "      ToTensor(),\n",
        "      Lambda(lambda y: y.flatten())\n",
        "  ])\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "  root=\"data\",\n",
        "  train=False,\n",
        "  download=True,\n",
        "  transform=Compose([ \n",
        "      ToTensor(),\n",
        "      Lambda(lambda y: y.flatten())\n",
        "  ])\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)\n",
        "\n",
        "class MyNeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyNeuralNetwork, self).__init__()\n",
        "    self.fc1 = nn.Linear(28*28, 512)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(512, 512)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.fc3 = nn.Linear(512, 10)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu2(x)\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "model = MyNeuralNetwork()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "KmGdrYBDpq_p"
      },
      "source": [
        "## Hyperparameters\n",
        "\n",
        "Hyperparameters are adjustable parameters that let you control the model optimization process. Different hyperparameter values can impact model training and convergence rates (read more about [hyperparameter](https://pytorch.org/tutorials/beginner/hyperparameter_tuning_tutorial.html) tuning)\n",
        "\n",
        "We define the following hyperparameters for training:\n",
        "* **Number of Epochs** - the number times to iterate over the dataset\n",
        "\n",
        "* **Batch Size** - the number of data samples propagated through the network before the parameters are updated\n",
        "\n",
        "* **Learning Rate** - how much to update models parameters at each batch/epoch.Smaller values yield slow learning speed, while large values may result in unpredictable behavior during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZohYDsBZppb8"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 5"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "GH_smu8Hp6W9"
      },
      "source": [
        "## Optimisation Loop\n",
        "\n",
        "Once we set our hyperparameters, we can then train and optimize our model with an optimization loop. Each iteration of the optimization loop is called an **epoch**.\n",
        "\n",
        "Each epoch consists of two main parts:\n",
        "* **The Train Loop** - iterate over the training dataset and try to converge to optimal parameters.\n",
        "\n",
        "* **The Validation/Test Loop** - iterate over the test dataset to check if model performance is improving.\n",
        "\n",
        "Let's briefly familiarize ourselves with some of the concepts used in the training loop."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Y60taB8xqKpw"
      },
      "source": [
        "### Loss Function\n",
        "\n",
        "\n",
        "When presented with some training data, our untrained network is likely not to give the correct answer. **Loss function** measures the degree of dissimilarity of obtained result to the target value, and it is the loss function that we want to minimize during training. To calculate the loss we make a prediction using the inputs of our given data sample and compare it against the true data label value.\n",
        "\n",
        "Common loss functions include ```nn.MSELoss``` (Mean Square Error) for regression tasks, and [nn.NLLLoss](https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html#torch.nn.NLLLoss) (Negative Log Likelihood) for classification. [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss) combines ```nn.LogSoftmax``` and ```nn.NLLLoss```.\n",
        "\n",
        "We pass our model's output logits to ```nn.CrossEntropyLoss```, which will normalize the logits and compute the prediction error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5FuXgJZRp5sJ"
      },
      "outputs": [],
      "source": [
        "# Initialize the loss function\n",
        "loss_fn = nn.CrossEntropyLoss()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OWebnOSKqh8N"
      },
      "source": [
        "### Optimiser\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Optimization is the process of adjusting model parameters to reduce model error in each training step. **Optimization algorithms** define how this process is performed (in this example we use Stochastic Gradient Descent). All optimization logic is encapsulated in the ```optimizer``` object. Here, we use the SGD optimizer; additionally, there are many [different optimizers](https://pytorch.org/docs/stable/optim.html) available in PyTorch such as ADAM and RMSProp, that work better for different kinds of models and data.\n",
        "\n",
        "We initialize the optimizer by registering the model's parameters that need to be trained, and passing in the learning rate hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "GPyC2hFvqgXq",
        "outputId": "8782948f-3e8d-475d-c9fb-43358663294b"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "CiDjHf0UqzTM"
      },
      "source": [
        "Inside the training loop, optimization happens in three steps:\n",
        "* Call ```optimizer.zero_grad()``` to reset the gradients of model parameters. Gradients by default add up; to prevent double-counting, we explicitly zero them at each iteration.\n",
        "\n",
        "\n",
        "\n",
        "* Backpropagate the prediction loss with a call to ```loss.backward()```. PyTorch deposits the gradients of the loss w.r.t. each parameter.\n",
        "\n",
        "* Once we have our gradients, we call ```optimizer.step()``` to adjust the parameters by the gradients collected in the backward pass."
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "piT6RZJ7rNZn"
      },
      "source": [
        "## Wrapping it up\n",
        "\n",
        "\n",
        "We define ```train_loop``` that loops over our optimization code, and ```test_loop``` that evaluates the model's performance against our test data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "02K7CVmQqyFM"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "  size = len(dataloader.dataset)\n",
        "  for batch, (X, y) in enumerate(dataloader):\n",
        "    # Compute prediction and loss\n",
        "    pred = model(X)\n",
        "    loss = loss_fn(pred, y)\n",
        "\n",
        "    # Backpropagation\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch % 100 == 0:\n",
        "      loss, current = loss.item(), (batch + 1) * len(X)\n",
        "      print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "  # torch save model with torch.save()\n",
        "  torch.save({'model_weights': model.state_dict()}, 'model.pt')\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "  size = len(dataloader.dataset)\n",
        "  num_batches = len(dataloader)\n",
        "  test_loss, correct = 0, 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, y in dataloader:\n",
        "      pred = model(X)\n",
        "      test_loss += loss_fn(pred, y).item()\n",
        "      correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "  test_loss /= num_batches\n",
        "  correct /= size\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "fNaW_97Mrd2T"
      },
      "source": [
        "We initialize the loss function and optimizer, and pass it to ```train_loop``` and ```test_loop```. Feel free to increase the number of epochs to track the model's improving performance.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0EZir8Q7rcxN",
        "outputId": "1bf793ea-f7bf-4aa6-be0c-070b14f950bf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.296042  [   64/60000]\n",
            "loss: 2.295827  [ 6464/60000]\n",
            "loss: 2.272335  [12864/60000]\n",
            "loss: 2.271542  [19264/60000]\n",
            "loss: 2.259816  [25664/60000]\n",
            "loss: 2.216730  [32064/60000]\n",
            "loss: 2.232727  [38464/60000]\n",
            "loss: 2.195105  [44864/60000]\n",
            "loss: 2.202378  [51264/60000]\n",
            "loss: 2.177230  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 40.7%, Avg loss: 2.166577 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.174120  [   64/60000]\n",
            "loss: 2.172288  [ 6464/60000]\n",
            "loss: 2.117008  [12864/60000]\n",
            "loss: 2.137527  [19264/60000]\n",
            "loss: 2.092700  [25664/60000]\n",
            "loss: 2.018604  [32064/60000]\n",
            "loss: 2.061072  [38464/60000]\n",
            "loss: 1.980174  [44864/60000]\n",
            "loss: 1.996942  [51264/60000]\n",
            "loss: 1.937052  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 57.9%, Avg loss: 1.928493 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.953500  [   64/60000]\n",
            "loss: 1.934546  [ 6464/60000]\n",
            "loss: 1.827004  [12864/60000]\n",
            "loss: 1.872889  [19264/60000]\n",
            "loss: 1.760499  [25664/60000]\n",
            "loss: 1.696632  [32064/60000]\n",
            "loss: 1.735917  [38464/60000]\n",
            "loss: 1.631169  [44864/60000]\n",
            "loss: 1.664209  [51264/60000]\n",
            "loss: 1.560260  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.5%, Avg loss: 1.573183 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.634500  [   64/60000]\n",
            "loss: 1.602793  [ 6464/60000]\n",
            "loss: 1.459544  [12864/60000]\n",
            "loss: 1.528192  [19264/60000]\n",
            "loss: 1.399690  [25664/60000]\n",
            "loss: 1.385861  [32064/60000]\n",
            "loss: 1.405269  [38464/60000]\n",
            "loss: 1.327915  [44864/60000]\n",
            "loss: 1.365273  [51264/60000]\n",
            "loss: 1.255309  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.3%, Avg loss: 1.286132 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 1.367316  [   64/60000]\n",
            "loss: 1.346617  [ 6464/60000]\n",
            "loss: 1.186366  [12864/60000]\n",
            "loss: 1.283675  [19264/60000]\n",
            "loss: 1.155704  [25664/60000]\n",
            "loss: 1.173596  [32064/60000]\n",
            "loss: 1.193788  [38464/60000]\n",
            "loss: 1.133562  [44864/60000]\n",
            "loss: 1.172083  [51264/60000]\n",
            "loss: 1.077415  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.9%, Avg loss: 1.105726 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "TOlsKibwvp_O"
      },
      "source": [
        "Loading the model is easy!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fgx4_FaluxiV",
        "outputId": "32d5026a-583e-48a2-f10b-1fcf372db50d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = MyNeuralNetwork()\n",
        "\n",
        "checkpoint = torch.load('model.pt')\n",
        "model.load_state_dict(checkpoint['model_weights'])"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OYKK1PC_yfAo"
      },
      "source": [
        "# Accessing layer weights and biases\n",
        "\n",
        "We can also check the weights and biases a specific layer has after it has been trained"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FrouYIx3vrvy",
        "outputId": "cb3bf662-011d-4003-a8d7-056b70f9331c"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Parameter containing:\n",
              " tensor([[ 0.0172, -0.0330, -0.0139,  ..., -0.0307,  0.0187, -0.0117],\n",
              "         [-0.0344, -0.0109, -0.0038,  ...,  0.0232,  0.0260, -0.0013],\n",
              "         [ 0.0008,  0.0175, -0.0088,  ..., -0.0121,  0.0317, -0.0088],\n",
              "         ...,\n",
              "         [-0.0242,  0.0182, -0.0078,  ...,  0.0314, -0.0098,  0.0131],\n",
              "         [-0.0107, -0.0216, -0.0345,  ...,  0.0243, -0.0118, -0.0037],\n",
              "         [-0.0137, -0.0282, -0.0202,  ..., -0.0283, -0.0161,  0.0152]],\n",
              "        requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([ 5.8582e-03,  2.9430e-02, -3.3531e-02,  2.6839e-02, -2.1593e-02,\n",
              "         -4.1716e-03,  2.4195e-02, -1.6125e-02,  1.6232e-03,  3.6439e-02,\n",
              "         -2.3043e-02, -1.3096e-02,  2.1101e-02, -1.2119e-02,  2.5329e-02,\n",
              "         -1.5096e-02, -7.2799e-03,  2.8381e-02, -1.6743e-02, -8.7338e-03,\n",
              "         -8.8130e-03,  1.0894e-02, -2.4096e-02,  2.7904e-02,  2.3306e-02,\n",
              "          3.7503e-02,  3.7143e-02, -3.5524e-02,  2.3372e-02,  1.5238e-02,\n",
              "          3.3955e-02,  2.7311e-02,  3.2562e-02, -2.6288e-02,  2.8137e-02,\n",
              "         -1.8158e-02,  2.6906e-02, -4.9884e-03,  1.3264e-02, -1.6650e-03,\n",
              "          2.8769e-02,  2.7590e-02,  2.3992e-02, -1.9580e-02,  2.5057e-02,\n",
              "         -1.4195e-02,  1.3476e-02, -1.1890e-02, -2.4702e-02,  3.1806e-02,\n",
              "          3.3634e-02,  2.1452e-02, -1.0312e-02, -1.7711e-02, -1.6084e-02,\n",
              "          3.4565e-02,  2.1111e-02, -1.0406e-02, -2.0540e-02,  4.3051e-02,\n",
              "          2.6054e-02,  1.7012e-02, -1.3929e-04,  3.1349e-02,  1.7051e-02,\n",
              "          2.0629e-02, -3.3332e-02, -2.8740e-02, -1.8371e-02, -2.9097e-02,\n",
              "          2.1931e-03, -2.3702e-02, -1.0834e-02,  2.1899e-02,  1.8722e-02,\n",
              "         -2.0476e-02, -8.4775e-03, -2.6227e-02,  1.7800e-02,  3.2954e-02,\n",
              "         -3.0679e-02, -3.2489e-02,  2.0859e-02,  3.1334e-02,  1.5547e-02,\n",
              "          6.8278e-03,  1.5259e-03,  5.5144e-04,  2.0518e-02,  1.4960e-02,\n",
              "         -3.3087e-02, -2.1537e-02,  1.8390e-02, -2.7388e-02,  2.6156e-02,\n",
              "         -2.1981e-02, -1.1608e-02,  2.7270e-02,  1.7859e-02,  2.7165e-02,\n",
              "         -2.0451e-02,  6.1935e-03,  2.3548e-02,  1.3315e-03,  2.7525e-02,\n",
              "          1.2193e-02,  1.9628e-02, -2.2799e-02,  2.0735e-02, -7.7867e-03,\n",
              "         -1.5694e-02,  3.2125e-03,  6.7833e-03, -2.3845e-02,  7.0904e-03,\n",
              "          1.4820e-02,  2.2995e-02, -8.5544e-03,  1.3835e-02, -1.0788e-02,\n",
              "         -1.8659e-03,  1.3020e-02, -6.6261e-03, -2.3849e-02, -7.8028e-03,\n",
              "         -9.2062e-03, -3.2581e-02, -3.3058e-03,  3.5305e-02,  2.5710e-02,\n",
              "          2.1437e-02,  4.8980e-03, -6.8046e-03,  2.3489e-02, -2.4998e-02,\n",
              "          1.3127e-02, -3.1595e-02,  2.2378e-02,  3.9408e-02,  6.3698e-03,\n",
              "          3.0111e-02,  2.0201e-02, -2.3507e-02, -1.5308e-02, -8.9429e-03,\n",
              "         -9.0412e-03,  1.8619e-02,  5.8352e-03,  1.7404e-02,  1.0773e-02,\n",
              "          8.2271e-03, -4.2354e-03,  1.6895e-02, -5.2138e-03, -2.6711e-02,\n",
              "         -2.5456e-02,  2.3163e-02,  2.0735e-02, -6.8647e-03, -2.0576e-02,\n",
              "          2.1083e-02,  4.2821e-04, -1.2017e-02,  1.6391e-02, -3.2192e-02,\n",
              "          9.5506e-03, -2.6687e-02,  6.2230e-03,  9.9557e-03, -8.9081e-03,\n",
              "         -2.0591e-02, -2.0824e-02,  2.8474e-02, -3.4364e-02, -1.6728e-03,\n",
              "          5.3709e-03,  7.2896e-03, -2.4422e-02, -2.0014e-02,  3.1251e-02,\n",
              "          6.8690e-03,  2.4826e-02,  4.1970e-02, -3.5038e-03,  2.4840e-03,\n",
              "         -3.3725e-02, -3.0051e-02, -2.2985e-02, -1.7776e-03,  2.8650e-02,\n",
              "          3.6206e-02,  8.4258e-03,  3.2438e-02, -2.7815e-02,  1.5101e-02,\n",
              "          1.8648e-02,  3.2421e-02,  3.6316e-03, -2.4595e-02,  2.3375e-02,\n",
              "         -8.3565e-03, -1.8159e-02,  1.5632e-02, -1.6877e-02,  1.6408e-03,\n",
              "          1.9306e-02, -3.4317e-02, -2.2796e-02, -2.5843e-02,  8.7880e-03,\n",
              "         -2.7042e-02, -2.1462e-02,  1.9129e-03,  3.7661e-02,  1.8436e-02,\n",
              "          3.6199e-02, -2.5034e-02,  3.6065e-03, -5.1879e-03, -2.5810e-02,\n",
              "         -7.2809e-03,  5.5151e-03, -8.4053e-03, -1.5413e-02,  2.0668e-02,\n",
              "         -2.5594e-02,  1.2882e-02,  1.3686e-02,  1.0912e-02,  4.3715e-03,\n",
              "         -1.4843e-02, -2.7248e-03, -2.2174e-02, -3.4131e-02,  3.3566e-02,\n",
              "         -9.4672e-03, -2.5365e-02, -1.1459e-02, -1.8228e-02,  2.7401e-02,\n",
              "          1.7208e-02, -1.3719e-02,  3.2414e-02,  1.9590e-02,  2.6436e-02,\n",
              "          2.2167e-02, -2.0717e-02, -1.4681e-02,  4.1514e-02,  2.0524e-02,\n",
              "         -2.2175e-02,  3.4182e-02,  3.0602e-02,  2.7729e-03, -7.1430e-03,\n",
              "          3.4223e-03,  4.3400e-03,  2.8683e-02, -3.0342e-02,  1.9875e-04,\n",
              "         -3.0495e-02,  8.2541e-03,  1.5983e-02, -2.7105e-02,  3.7944e-03,\n",
              "         -2.3884e-02, -1.6497e-02, -1.3645e-02, -4.5579e-03, -3.6430e-03,\n",
              "         -1.3761e-03,  4.1500e-02,  3.3261e-02, -4.8171e-03, -1.3718e-02,\n",
              "          2.0951e-02, -2.5675e-02,  1.1525e-02,  1.5220e-02,  2.6788e-02,\n",
              "         -1.2709e-03, -1.1414e-03, -2.3785e-02, -7.7589e-03,  3.8573e-02,\n",
              "         -2.4496e-03, -6.4411e-03,  1.0160e-02, -1.3581e-02, -2.0392e-02,\n",
              "         -1.1029e-02, -2.8869e-03,  1.8700e-02, -5.4076e-03,  2.6236e-03,\n",
              "         -2.1237e-02, -1.1467e-03,  2.9431e-02, -2.2549e-02,  4.3499e-03,\n",
              "         -9.0254e-03, -1.2156e-02,  2.2777e-02,  2.8054e-02,  1.6362e-02,\n",
              "         -1.3596e-02, -2.6220e-02,  3.8160e-02,  9.9457e-03, -2.9307e-02,\n",
              "         -9.4068e-03, -2.1537e-02,  1.9506e-02,  6.9392e-03,  1.5296e-02,\n",
              "         -2.2089e-02,  1.3068e-02,  1.2544e-02, -4.4827e-03, -1.3743e-02,\n",
              "          2.4014e-02,  2.8330e-02, -1.4338e-02,  3.6999e-03,  2.8028e-02,\n",
              "         -2.1839e-02,  1.5410e-02,  1.9938e-03,  1.0678e-02,  2.9994e-02,\n",
              "          1.3387e-02,  1.1122e-02, -2.5778e-02,  1.0837e-02, -1.4635e-02,\n",
              "          1.2246e-02, -6.3712e-03, -2.9178e-02,  2.3548e-02,  2.3074e-02,\n",
              "         -7.8166e-03, -2.4092e-02, -7.5483e-03, -2.7046e-02, -2.0128e-03,\n",
              "         -3.6068e-03, -2.6987e-03, -4.7049e-03, -3.5513e-02, -3.1436e-02,\n",
              "         -1.2079e-02,  2.8838e-03,  8.5263e-03,  3.0807e-02,  2.6271e-02,\n",
              "          2.9576e-02,  3.9316e-02, -1.1584e-02,  2.4267e-02,  6.2632e-03,\n",
              "          3.8866e-02, -1.8782e-02, -2.2911e-02, -2.0016e-02, -4.1805e-03,\n",
              "          2.5446e-02, -1.5192e-02,  1.4290e-02,  1.3235e-02,  4.7932e-02,\n",
              "          2.7652e-02, -2.7006e-02,  2.4831e-02, -2.3805e-02, -3.3053e-02,\n",
              "         -3.3030e-02, -2.8943e-02, -3.0364e-02, -9.3636e-03,  1.4739e-02,\n",
              "          3.6008e-02,  2.5796e-02, -3.4046e-03,  2.4447e-02, -6.5100e-03,\n",
              "          2.5122e-02, -2.0501e-03, -2.8484e-02,  1.3313e-02,  1.8502e-02,\n",
              "          1.5713e-02, -2.4937e-02, -2.1869e-02,  1.3302e-02,  2.0958e-02,\n",
              "         -2.1637e-02, -2.2066e-02, -2.6874e-02, -2.6557e-02,  1.1727e-02,\n",
              "         -1.8900e-02, -1.2741e-02, -3.2284e-02, -3.3894e-02,  1.3060e-02,\n",
              "          3.1869e-02, -5.6281e-03,  2.5895e-02,  6.4769e-03, -2.4437e-02,\n",
              "         -7.8479e-03, -2.0227e-02,  2.0312e-02,  3.0143e-02,  1.2015e-02,\n",
              "          8.0263e-03,  1.7769e-02,  1.5211e-02,  1.1022e-02,  6.1619e-03,\n",
              "         -1.1683e-02,  2.6132e-02,  2.5981e-02,  1.0997e-02, -2.6892e-02,\n",
              "         -2.0293e-02,  3.2627e-02, -1.4728e-02, -1.3402e-02, -1.9353e-02,\n",
              "          2.4271e-02,  1.3273e-02,  2.1334e-02,  6.4567e-03,  7.0384e-03,\n",
              "          1.2627e-02,  1.4407e-02, -2.3342e-02,  5.8638e-03, -2.2471e-02,\n",
              "          1.1557e-02, -2.9716e-02, -7.4248e-04,  5.9584e-03,  2.0224e-03,\n",
              "          9.9454e-04,  1.0509e-02, -8.8997e-03, -8.7368e-03, -5.5773e-05,\n",
              "          3.6914e-03,  3.3986e-03,  2.2007e-02, -1.4600e-02, -7.7884e-03,\n",
              "          3.5507e-02,  7.9048e-03,  2.3482e-02, -1.4072e-03, -2.7920e-02,\n",
              "          2.8933e-02,  3.8655e-02,  1.6001e-02,  9.8351e-03,  2.9379e-02,\n",
              "         -1.2947e-02, -3.0375e-02, -2.7257e-02,  1.1606e-02, -1.5147e-02,\n",
              "          3.7628e-02,  3.1902e-02,  1.9455e-02,  3.4262e-02,  1.2021e-02,\n",
              "         -1.2065e-02,  1.6149e-02, -3.6835e-03, -1.1001e-02,  1.6925e-02,\n",
              "          3.7699e-02,  2.2913e-02, -2.9210e-02,  9.2528e-03, -1.5142e-03,\n",
              "         -2.6728e-02,  8.7660e-03, -3.2459e-02,  2.0104e-02,  1.1664e-02,\n",
              "          2.4513e-02,  4.3466e-02,  1.5885e-02, -8.1938e-03, -2.0617e-02,\n",
              "          1.8032e-03, -1.9659e-02,  2.1545e-02, -3.4084e-02, -2.0405e-02,\n",
              "          9.3878e-03,  2.7980e-02,  1.7522e-02,  2.9184e-02,  2.7365e-02,\n",
              "          7.8054e-03, -6.4783e-05, -1.3135e-02, -2.1349e-02,  4.5922e-02,\n",
              "          8.1978e-03, -1.3303e-03], requires_grad=True))"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fc1.weight, model.fc1.bias"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "EMs5ns9Yyxq_"
      },
      "source": [
        "## Freezing a layer prior to training\n",
        "\n",
        "Now, we can freeze a specific layer, train the model, and verify whether the weights and biases remain unchanged"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "f3EoZN6nvujU"
      },
      "outputs": [],
      "source": [
        "model = MyNeuralNetwork() # reset the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJvbxhezzWae",
        "outputId": "9c32ef0d-4c4d-4aa2-8ad8-e431dc10c255"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Linear(in_features=784, out_features=512, bias=True)"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fc1.requires_grad_(False) # freeze the first linear layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--97RO3Uza1I",
        "outputId": "49905f0c-4861-4b67-d3f0-614e36361a5d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer fc1.weight requires grad = False\n",
            "Layer fc1.bias requires grad = False\n",
            "Layer fc2.weight requires grad = True\n",
            "Layer fc2.bias requires grad = True\n",
            "Layer fc3.weight requires grad = True\n",
            "Layer fc3.bias requires grad = True\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "  print(f'Layer {name} requires grad = {param.requires_grad}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "hXC2WURqzwh1"
      },
      "outputs": [],
      "source": [
        "weights_before_train, bias_before_train = model.fc1.weight, model.fc1.bias"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "wo-8ilyNzu41"
      },
      "source": [
        "Let's retrain the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mo6ZHC3ezlWr",
        "outputId": "22d14af8-5ee9-40ba-8740-f1af7fdcbd6f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.299551  [   64/60000]\n",
            "loss: 2.296839  [ 6464/60000]\n",
            "loss: 2.289687  [12864/60000]\n",
            "loss: 2.286998  [19264/60000]\n",
            "loss: 2.279656  [25664/60000]\n",
            "loss: 2.271916  [32064/60000]\n",
            "loss: 2.264755  [38464/60000]\n",
            "loss: 2.253534  [44864/60000]\n",
            "loss: 2.253704  [51264/60000]\n",
            "loss: 2.238899  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 44.5%, Avg loss: 2.240959 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.242520  [   64/60000]\n",
            "loss: 2.241128  [ 6464/60000]\n",
            "loss: 2.220755  [12864/60000]\n",
            "loss: 2.229986  [19264/60000]\n",
            "loss: 2.219749  [25664/60000]\n",
            "loss: 2.196443  [32064/60000]\n",
            "loss: 2.208477  [38464/60000]\n",
            "loss: 2.184458  [44864/60000]\n",
            "loss: 2.192126  [51264/60000]\n",
            "loss: 2.171780  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 55.7%, Avg loss: 2.173741 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.182100  [   64/60000]\n",
            "loss: 2.180009  [ 6464/60000]\n",
            "loss: 2.146249  [12864/60000]\n",
            "loss: 2.166101  [19264/60000]\n",
            "loss: 2.150354  [25664/60000]\n",
            "loss: 2.116339  [32064/60000]\n",
            "loss: 2.140624  [38464/60000]\n",
            "loss: 2.105467  [44864/60000]\n",
            "loss: 2.118677  [51264/60000]\n",
            "loss: 2.091691  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 59.1%, Avg loss: 2.095114 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.109497  [   64/60000]\n",
            "loss: 2.105372  [ 6464/60000]\n",
            "loss: 2.058386  [12864/60000]\n",
            "loss: 2.088765  [19264/60000]\n",
            "loss: 2.063444  [25664/60000]\n",
            "loss: 2.024422  [32064/60000]\n",
            "loss: 2.054810  [38464/60000]\n",
            "loss: 2.010312  [44864/60000]\n",
            "loss: 2.027965  [51264/60000]\n",
            "loss: 1.993494  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 61.9%, Avg loss: 1.999914 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.020954  [   64/60000]\n",
            "loss: 2.013637  [ 6464/60000]\n",
            "loss: 1.952618  [12864/60000]\n",
            "loss: 1.994904  [19264/60000]\n",
            "loss: 1.956236  [25664/60000]\n",
            "loss: 1.918474  [32064/60000]\n",
            "loss: 1.950076  [38464/60000]\n",
            "loss: 1.899067  [44864/60000]\n",
            "loss: 1.921301  [51264/60000]\n",
            "loss: 1.878492  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.9%, Avg loss: 1.889832 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "C21A1H7Az8nY"
      },
      "outputs": [],
      "source": [
        "weights_after_train, bias_after_train = model.fc1.weight, model.fc1.bias"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "OBy47lMn0wGO"
      },
      "source": [
        "The two weight matrices and bias vectors should be ```True```\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iDnCybNH0YqO",
        "outputId": "23bc79dd-be0a-4071-a2dc-51ad33a89de2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.equal(weights_after_train, weights_before_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFSh-V9H0eMY",
        "outputId": "e6fdd163-5331-4945-9e07-ac3d3e286f51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.equal(bias_before_train, bias_after_train)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "232A_kSV08cD"
      },
      "source": [
        "## Initialising layer weights and biases\n",
        "\n",
        "The default initialisation for each layer follows a Uniform distribution.\n",
        "\n",
        "We can fine tune the weight initialisation for each layer as we please!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "SH6SfUj2047y"
      },
      "outputs": [],
      "source": [
        "class MyNeuralNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MyNeuralNetwork, self).__init__()\n",
        "    self.fc1 = nn.Linear(28*28, 512)\n",
        "    self.relu1 = nn.ReLU()\n",
        "    self.fc2 = nn.Linear(512, 512)\n",
        "    self.relu2 = nn.ReLU()\n",
        "    self.fc3 = nn.Linear(512, 10)\n",
        "\n",
        "    # simple weight initialisation\n",
        "    # for more complex models\n",
        "    # please use the function init_weights(m: nn.Module)\n",
        "    # and, after initialising your model,\n",
        "    # call model.apply(init_weights)\n",
        "    # https://stackoverflow.com/questions/49433936/how-do-i-initialize-weights-in-pytorch\n",
        "    torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
        "    torch.nn.init.orthogonal_(self.fc2.weight)\n",
        "    torch.nn.init.constant_(self.fc3.weight, 42)\n",
        "\n",
        "    torch.nn.init.constant_(self.fc3.bias.data, 15)\n",
        "    torch.nn.init.normal_(self.fc1.bias.data)\n",
        "    torch.nn.init.normal_(self.fc2.bias.data)\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.fc1(x)\n",
        "    x = self.relu1(x)\n",
        "    x = self.fc2(x)\n",
        "    x = self.relu2(x)\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "model = MyNeuralNetwork()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DK63Vu3K2Kcd",
        "outputId": "bda701d2-578d-4640-8cf3-9781a3d39565"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Parameter containing:\n",
              " tensor([[ 5.1275e-02, -1.3068e-03,  1.5896e-02,  ..., -4.9564e-02,\n",
              "           4.4105e-02, -6.1847e-03],\n",
              "         [-3.5086e-02,  2.4820e-02,  3.6459e-03,  ..., -3.0692e-02,\n",
              "           4.9835e-02, -5.1433e-02],\n",
              "         [ 2.5943e-02, -1.9529e-02,  3.4130e-02,  ..., -5.0271e-02,\n",
              "           4.1214e-02, -1.1813e-02],\n",
              "         ...,\n",
              "         [ 4.3373e-02,  7.4763e-04,  5.6291e-02,  ...,  2.0110e-02,\n",
              "          -2.9809e-02, -5.5251e-03],\n",
              "         [-1.8178e-02, -2.9096e-02,  3.7008e-02,  ..., -4.9279e-02,\n",
              "          -5.8290e-02, -5.4098e-02],\n",
              "         [ 3.4350e-03, -3.3557e-05,  5.4387e-02,  ...,  3.5376e-02,\n",
              "           3.5175e-02, -2.4830e-02]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-1.6181e-01, -9.2999e-01, -1.2743e+00, -2.8756e-01,  6.1159e-01,\n",
              "          1.0674e+00, -8.0036e-01, -5.6633e-01,  7.6102e-01,  5.5649e-01,\n",
              "         -1.5019e-03,  1.8640e+00,  1.0728e-01, -6.4115e-01,  2.2803e-01,\n",
              "          5.5686e-02, -3.0776e-01, -1.7295e+00, -6.9720e-01,  1.4416e+00,\n",
              "         -1.0766e+00,  5.5293e-01, -7.6556e-02,  1.3323e+00,  1.8131e+00,\n",
              "         -1.3988e+00,  2.2440e-01, -6.7653e-01,  7.8822e-01, -3.1961e-02,\n",
              "         -7.6735e-02,  6.8370e-01,  5.8010e-01,  2.8407e-01, -3.2623e-01,\n",
              "          1.6377e+00,  4.2263e-01,  7.6565e-01,  6.9099e-01, -5.9780e-01,\n",
              "         -1.8414e+00,  3.2881e-01, -3.4749e-01, -4.8811e-01,  1.8600e-01,\n",
              "          8.9904e-01,  6.6501e-01, -9.2328e-01,  9.8832e-03, -3.7174e-01,\n",
              "         -1.0359e-03,  6.5010e-01, -1.2025e+00,  1.7864e-01,  1.3629e-01,\n",
              "          1.4297e+00,  3.0364e+00, -6.8831e-01,  3.6094e-01,  1.1023e+00,\n",
              "          1.8214e+00,  6.7780e-01, -5.1326e-01, -4.6461e-02, -2.6636e-01,\n",
              "          1.3478e-01, -9.5145e-01,  5.6524e-01,  9.1654e-02,  2.8221e-01,\n",
              "         -8.1506e-01,  1.7833e+00,  1.1291e+00,  1.5912e-01, -7.3373e-01,\n",
              "         -3.1508e-02, -1.3777e+00, -3.8119e-01, -5.4421e-01,  8.2146e-01,\n",
              "         -2.5781e-01,  4.7770e-01, -2.0143e+00,  1.7965e-01,  2.5214e-02,\n",
              "         -1.8493e-01, -6.2896e-01, -1.3152e+00, -1.2241e-01,  3.7618e-01,\n",
              "          3.5041e-01, -2.2382e+00, -2.3725e+00, -4.4307e-01,  7.4497e-01,\n",
              "          1.0605e+00,  2.2497e-02,  8.9273e-01, -1.5734e-01, -5.1197e-01,\n",
              "         -3.1962e-01,  1.6942e+00, -1.1196e+00, -4.9505e-01, -5.4641e-02,\n",
              "          6.5658e-03,  7.3813e-01,  8.9147e-01,  1.4218e+00, -1.0043e-01,\n",
              "         -4.0031e-01,  6.0037e-01,  6.3708e-01,  1.3284e-01,  8.3246e-01,\n",
              "          5.2343e-01, -1.3173e-01,  3.6169e-01, -6.6221e-01,  4.5228e-01,\n",
              "         -3.0160e-01, -1.7954e-01, -2.6280e-01,  1.5000e-02,  3.4137e-01,\n",
              "         -1.3504e-01, -2.0128e-01, -2.0156e+00, -1.1328e+00, -1.3524e+00,\n",
              "          1.4379e-02,  2.3101e+00, -5.7899e-01, -1.5482e-01,  2.1945e-01,\n",
              "          2.9947e-02,  7.9624e-01,  4.1953e-01, -3.9444e-01, -1.2462e+00,\n",
              "         -4.3843e-01, -6.6382e-01, -1.0682e+00,  8.2477e-01,  1.2070e+00,\n",
              "         -2.7154e-01,  3.7973e-01,  7.2936e-01,  3.8367e-02,  3.8014e-01,\n",
              "         -5.9073e-01,  1.1356e+00, -2.5955e-02, -2.0464e-01, -7.1110e-01,\n",
              "         -1.8201e+00, -1.3719e-01,  1.0525e+00,  3.4224e-02,  3.3404e-01,\n",
              "         -1.6575e+00,  4.0713e-01,  1.4108e-01,  1.8006e-01,  3.4253e-01,\n",
              "          1.0828e+00,  3.9828e-01,  2.0971e+00,  9.4530e-01,  2.5575e+00,\n",
              "          1.0485e+00, -1.0402e-02,  7.4628e-01, -5.5630e-01,  8.4614e-01,\n",
              "         -1.6524e-01, -6.6607e-01, -6.9867e-01, -1.3610e+00, -4.2474e-02,\n",
              "          1.2408e+00, -1.4285e+00,  1.1795e+00,  5.2177e-02, -1.9191e+00,\n",
              "          1.6632e+00, -7.1458e-01,  3.1057e-01, -4.5685e-01,  6.3287e-01,\n",
              "          1.1851e+00, -1.9571e+00, -1.1111e+00,  2.7410e-01, -4.1879e-01,\n",
              "          1.1032e-01, -8.4165e-01, -1.1240e+00, -2.5348e-01, -8.0254e-01,\n",
              "          9.1022e-01, -3.8275e-01, -1.5536e-01,  5.1923e-01, -1.0963e+00,\n",
              "          3.5706e-01, -8.7586e-01,  8.7921e-01,  1.2315e+00, -1.4853e-01,\n",
              "         -1.8823e+00,  4.8121e-01,  5.7858e-01, -6.8583e-01, -1.7984e+00,\n",
              "          7.1859e-01,  1.9957e+00, -1.5387e-01,  2.3555e+00, -9.1519e-01,\n",
              "         -2.7907e-01, -1.9700e+00,  6.0955e-01, -1.0604e+00,  4.0236e-01,\n",
              "         -1.2574e+00, -1.9345e-01, -4.7982e-01, -9.4335e-01, -4.7231e-01,\n",
              "          5.4294e-01, -2.3589e+00,  3.0316e-01,  6.5717e-01, -1.1230e+00,\n",
              "          4.8571e-01, -2.6419e+00, -1.3521e+00,  1.0352e+00,  1.8642e-01,\n",
              "         -1.4860e-01, -1.1347e-02,  1.6058e+00, -6.9135e-01, -9.8123e-01,\n",
              "         -6.1779e-01,  3.4828e-02,  5.1362e-01, -8.7734e-01, -1.3730e-01,\n",
              "          5.8161e-01, -6.0005e-01,  4.3855e-01,  6.6107e-01,  4.7396e-02,\n",
              "          1.5523e-01, -1.6714e-01,  2.2768e-01, -1.0312e+00,  7.8283e-01,\n",
              "         -4.2997e-01, -3.2513e-01, -4.7159e-01,  1.7883e-01, -2.0921e-01,\n",
              "          7.7941e-01, -1.2924e+00,  1.0012e+00, -6.9743e-01, -1.0377e+00,\n",
              "          1.5090e+00,  5.8644e-01,  2.0490e-01,  2.1820e-01,  6.1396e-01,\n",
              "         -9.0033e-01,  1.3487e+00, -1.1279e+00, -3.4693e-01,  6.1587e-01,\n",
              "          9.9471e-01,  3.6181e-01, -8.7806e-01,  1.2800e+00, -1.5701e+00,\n",
              "         -6.3064e-02,  6.1967e-02, -4.2246e-01,  2.5262e-01,  6.6499e-03,\n",
              "         -7.8582e-01,  4.9751e-01,  5.7260e-01, -1.9315e+00,  2.8547e+00,\n",
              "         -2.0097e+00, -1.0258e-01, -9.2791e-01, -4.9889e-02,  6.7368e-02,\n",
              "          1.0801e+00, -1.2633e+00, -2.8958e-01, -1.6716e+00, -5.6269e-01,\n",
              "          2.4470e-02, -6.5589e-02, -5.1376e-01, -5.9454e-01,  2.8702e-01,\n",
              "         -3.6098e-01,  3.2471e-01, -1.0218e+00, -7.5546e-01,  4.3101e-01,\n",
              "          1.0729e+00, -8.6957e-01, -6.7552e-01,  6.2968e-01, -2.8854e-01,\n",
              "         -1.0943e-01,  1.7976e-01, -4.4071e-01, -8.1644e-01, -1.4081e+00,\n",
              "          8.6649e-01,  6.6874e-01,  2.1552e-01,  1.0529e+00,  3.8979e-01,\n",
              "          9.0828e-01, -2.0039e-01,  1.7870e+00,  3.9903e-01,  1.6301e+00,\n",
              "          1.9691e+00,  7.8946e-01,  7.4123e-01, -1.1332e+00, -1.7046e-01,\n",
              "          3.2321e-01,  7.5826e-01,  1.4487e+00, -1.1006e+00,  1.6967e+00,\n",
              "          1.2076e+00, -1.2879e-01,  8.5903e-01, -1.5933e-01, -2.2799e+00,\n",
              "         -9.0039e-01,  2.6669e-01,  1.0352e+00,  5.7639e-01, -6.3814e-01,\n",
              "         -4.0022e-01,  4.9878e-01,  1.6723e+00,  9.8532e-01,  4.3602e-01,\n",
              "          3.3012e-01,  1.1816e+00,  5.1205e-01, -1.0225e+00, -4.5837e-01,\n",
              "          1.3371e+00,  1.3801e+00,  9.9388e-01,  1.5156e+00,  1.2857e+00,\n",
              "          2.4061e+00,  4.3572e-01, -1.7289e+00,  1.2770e+00,  6.6098e-01,\n",
              "         -1.1136e+00, -1.0595e+00,  1.0599e+00,  9.1324e-01, -1.0047e+00,\n",
              "          2.4357e-01, -4.8470e-01, -1.5282e+00, -5.2746e-01,  1.9865e+00,\n",
              "          1.5784e+00,  9.9879e-01, -1.8512e+00, -9.3051e-02, -1.5296e+00,\n",
              "          3.2482e-01,  1.5274e+00, -1.0186e+00, -1.7681e+00, -6.7997e-01,\n",
              "         -9.5278e-01,  1.0872e+00, -6.8058e-01, -3.0302e-02, -6.6204e-01,\n",
              "         -6.3430e-01,  6.6373e-01,  1.6663e+00, -1.5136e+00,  1.2794e-01,\n",
              "          2.0523e-01,  1.2155e+00,  2.0564e-01,  1.6780e-01,  1.8040e+00,\n",
              "          2.8737e-01,  1.0737e+00, -9.4224e-01,  4.8271e-01, -1.0444e+00,\n",
              "          5.7309e-01, -4.0168e-01,  7.3024e-03, -9.3419e-01,  6.7580e-01,\n",
              "         -8.1831e-01, -1.1090e+00,  1.8111e+00,  4.8170e-01,  6.3087e-02,\n",
              "          1.3532e+00, -7.9974e-01,  1.7220e-01,  2.3594e-01, -7.5201e-01,\n",
              "          5.2517e-01, -3.8643e-01, -5.6307e-01,  4.6990e-01, -1.3327e+00,\n",
              "         -5.8201e-01, -2.9944e-02,  5.4304e-01,  9.1471e-01, -9.8589e-02,\n",
              "         -4.8535e-01,  7.8435e-01,  7.3158e-01, -3.8272e-01,  4.9818e-01,\n",
              "          6.5516e-01, -5.5758e-01,  2.0184e-01, -5.4953e-01,  9.5595e-01,\n",
              "          3.4490e-01, -2.1310e-01, -1.1115e+00,  3.5917e-02,  5.7398e-01,\n",
              "          2.5652e+00,  1.5447e-01, -1.1688e-01, -4.4340e-01, -1.2896e+00,\n",
              "         -8.9100e-02, -2.2287e+00, -2.3425e+00,  7.5084e-01, -5.4347e-01,\n",
              "         -3.5169e-01,  3.9667e-01, -1.2857e+00,  2.1497e-01, -8.3904e-01,\n",
              "          9.9675e-01, -1.1240e+00, -5.8613e-01,  1.2947e+00,  1.0419e+00,\n",
              "          2.4131e-01,  8.8101e-02,  7.3200e-01,  4.4722e-01, -1.5821e-01,\n",
              "         -3.7606e-01,  8.3423e-01,  1.0640e+00, -9.8510e-03, -5.9298e-01,\n",
              "         -1.8928e+00,  1.5244e+00, -9.5328e-01, -3.6856e-02, -1.0828e+00,\n",
              "          1.7191e+00, -1.8655e+00,  1.1881e+00, -1.4318e+00,  7.3122e-01,\n",
              "         -9.5840e-02,  1.5732e+00, -1.7847e+00, -1.6935e+00, -9.4110e-01,\n",
              "          4.3289e-01, -9.2847e-01,  3.9131e-01, -1.5760e+00, -8.8343e-01,\n",
              "         -4.7972e-01,  1.6262e+00,  1.7408e-02, -2.6535e+00,  2.6733e-01,\n",
              "          6.5554e-01, -3.6322e-01], requires_grad=True))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fc1.weight, model.fc1.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kP84cj4_2kje",
        "outputId": "221f439c-1338-43d9-96dd-fd39de480569"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Parameter containing:\n",
              " tensor([[ 2.7968e-02,  4.4823e-02,  1.8171e-03,  ..., -3.3453e-02,\n",
              "          -7.1288e-02, -1.3297e-03],\n",
              "         [ 5.6843e-02, -5.4169e-04, -3.8008e-02,  ..., -9.9231e-03,\n",
              "           1.8337e-02,  3.3371e-02],\n",
              "         [-2.2703e-02,  6.6771e-02,  2.6552e-02,  ...,  2.4233e-02,\n",
              "          -1.3982e-01, -1.3285e-04],\n",
              "         ...,\n",
              "         [-2.9536e-02, -2.0175e-02, -6.3886e-02,  ..., -3.1870e-02,\n",
              "           1.0092e-01, -1.2971e-04],\n",
              "         [ 4.4442e-02,  1.2103e-02, -3.8644e-02,  ..., -3.1490e-02,\n",
              "          -2.4474e-02,  1.5977e-02],\n",
              "         [ 4.1921e-02, -2.7832e-02, -2.6377e-02,  ..., -1.2528e-02,\n",
              "          -1.5423e-02, -2.4361e-02]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([-1.3825e-01, -1.2891e+00,  8.2745e-01,  4.5596e-01,  4.2847e-01,\n",
              "          2.4569e-01,  1.0405e+00, -5.0403e-01,  2.9479e-01, -1.3895e+00,\n",
              "         -3.0098e-01,  5.6779e-01,  4.6672e-01,  1.1677e+00,  8.3628e-01,\n",
              "         -2.0265e+00, -6.0952e-01, -1.4919e+00,  4.2629e-02,  2.6962e-01,\n",
              "         -1.1972e-01, -8.2999e-01, -1.7197e+00, -9.3664e-01,  3.3654e-01,\n",
              "          2.3485e-01, -6.8771e-01, -1.9077e-01,  2.5963e+00, -1.1499e+00,\n",
              "          6.1313e-01, -6.4630e-01,  5.6486e-01,  1.1539e+00,  1.7608e+00,\n",
              "          1.8040e-01, -5.4855e-01,  8.2739e-01, -1.2338e+00, -1.0897e+00,\n",
              "         -2.3543e-01,  5.3606e-01, -8.8398e-01,  5.7985e-01,  1.0612e+00,\n",
              "          3.5433e-01, -2.4894e-01, -7.0458e-01,  1.0460e+00, -3.4460e-01,\n",
              "         -2.0374e+00, -5.8782e-01,  6.7101e-01, -2.9934e-01,  9.1944e-01,\n",
              "         -1.5807e+00,  3.0028e-01,  5.0793e-01, -1.4142e+00,  2.0676e-01,\n",
              "         -2.8466e+00,  4.0805e-02,  2.9541e-01,  1.7828e+00,  1.5347e+00,\n",
              "          8.2950e-01, -1.1201e+00,  9.6580e-01, -3.1706e-01, -6.8978e-01,\n",
              "         -1.3089e+00, -7.9451e-01, -3.5234e-01,  1.3404e-01, -2.9873e-01,\n",
              "          4.7976e-01, -4.3217e-01, -6.5793e-01, -1.0492e+00, -3.9366e-01,\n",
              "          1.1904e+00, -2.1693e-01, -6.2858e-01, -5.5964e-02,  8.2166e-02,\n",
              "         -7.5342e-01, -6.8257e-01,  2.4348e+00,  4.4204e-02, -1.7925e+00,\n",
              "          1.1577e+00, -4.9941e-01,  8.7867e-01, -3.2433e-03, -5.5204e-01,\n",
              "         -2.1084e-01, -1.2091e-01,  3.9229e-01, -8.4709e-01,  8.5395e-01,\n",
              "          7.2565e-01, -6.4719e-01,  4.5758e-01,  1.3179e+00,  5.0077e-01,\n",
              "         -3.3200e+00, -1.0846e+00, -1.9870e-02, -7.9782e-01, -3.4655e-01,\n",
              "         -8.6985e-01, -5.2895e-01,  1.2288e+00, -3.6455e-01,  9.9760e-01,\n",
              "          4.5463e-01, -3.3777e-01,  6.6017e-01,  1.5263e-01, -1.4785e-01,\n",
              "         -1.8237e+00, -1.5596e+00,  4.2610e-01, -1.0756e+00, -1.2967e+00,\n",
              "         -6.2914e-01, -5.3716e-01,  1.6835e+00, -3.4087e-01,  9.0378e-02,\n",
              "          1.9216e+00, -1.5004e+00,  1.1798e-01,  2.5325e-02, -9.8786e-01,\n",
              "          1.2841e-01,  3.9761e-01,  1.9931e+00,  2.2219e-01,  3.3553e-01,\n",
              "          1.9338e-01, -4.9499e-01,  5.4586e-01, -3.4533e-01,  7.7874e-01,\n",
              "          1.3829e+00,  1.0250e+00,  1.4395e+00,  7.9110e-01,  4.8347e-01,\n",
              "         -6.8295e-01, -5.3366e-01, -1.2539e+00, -7.6781e-01, -2.1093e+00,\n",
              "         -4.2877e-01, -6.0728e-02, -8.6172e-01,  1.7986e+00, -3.4204e-02,\n",
              "          7.2940e-01,  1.7265e+00, -1.7666e+00,  5.1236e-02,  8.3146e-01,\n",
              "          9.9992e-01, -7.3043e-01, -3.6794e-01,  6.0255e-01,  8.5544e-01,\n",
              "         -1.9262e-01, -3.8362e-01,  4.5105e-01, -2.3248e+00, -1.1724e+00,\n",
              "         -6.0894e-02,  1.8993e-01,  7.7638e-01,  2.8746e-01, -9.7160e-01,\n",
              "          1.4957e+00, -1.1053e+00,  1.7111e-01, -3.4502e-01, -8.7224e-01,\n",
              "          1.6709e-01,  4.8393e-02,  1.1449e-01,  3.2055e-02,  3.5122e-01,\n",
              "         -1.0333e+00, -3.6805e-01, -2.0206e+00, -1.6102e+00, -3.6805e-01,\n",
              "          6.5002e-01, -2.0920e+00,  1.4166e-01,  9.2331e-01, -3.1030e-02,\n",
              "         -4.9449e-01,  1.9925e+00,  7.3952e-01, -5.3203e-01,  1.3103e-01,\n",
              "         -1.4136e+00, -1.1897e+00,  1.1389e+00, -7.9386e-01, -2.0378e+00,\n",
              "         -1.9779e-01,  1.6581e-01, -1.1396e+00,  8.5629e-01, -1.2649e+00,\n",
              "          3.5502e-01, -1.6733e+00, -1.8432e+00,  1.4056e-01,  6.1831e-01,\n",
              "         -1.3638e+00, -2.2417e-01, -1.2753e+00, -1.1557e+00, -1.9449e+00,\n",
              "          9.2413e-01, -6.6964e-02,  1.7548e-01,  5.6323e-01, -2.6352e-02,\n",
              "          2.1142e+00,  3.1523e-01,  6.8640e-01, -1.3358e+00, -1.1846e+00,\n",
              "         -1.4593e+00, -8.9428e-01, -2.0446e-01, -1.6218e+00,  1.7505e+00,\n",
              "         -3.6420e-01, -1.0738e+00,  3.4057e-01,  7.6130e-01,  1.5153e+00,\n",
              "         -4.3994e-01,  6.4458e-01, -9.1218e-01,  6.1605e-01,  6.3838e-01,\n",
              "          9.5808e-01,  1.7118e-01, -4.3570e-01,  2.8671e-01, -7.7294e-02,\n",
              "          6.7473e-01, -1.4633e+00, -1.7873e+00, -3.4433e-01,  6.0237e-01,\n",
              "          1.0618e+00,  2.1546e+00,  5.9952e-02,  1.1147e+00, -4.8471e-01,\n",
              "          1.5901e-01,  9.6645e-01,  1.0026e-01, -2.0649e-01, -7.6131e-01,\n",
              "          4.0576e-01,  1.3755e+00,  4.9842e-01, -5.3381e-01, -2.0410e-01,\n",
              "         -8.5986e-01,  2.0241e-01, -4.0152e-02,  1.3369e+00, -4.7664e-02,\n",
              "         -1.8996e+00, -9.0086e-01,  1.4625e+00,  5.6979e-01, -8.9586e-01,\n",
              "         -4.9741e-01,  1.7548e-01,  3.7377e-01, -1.0909e+00,  1.3275e-01,\n",
              "         -1.0479e+00, -3.6608e-01,  1.6806e-02,  8.9993e-01, -1.1312e+00,\n",
              "          1.1955e+00,  3.8844e-01,  5.7964e-01, -2.7193e-01,  1.3278e+00,\n",
              "          1.2728e+00,  1.2172e+00, -1.2393e+00,  7.2443e-01, -5.8676e-01,\n",
              "         -3.1559e+00,  6.9530e-01,  5.7879e-01, -8.6225e-01, -2.9419e-01,\n",
              "          1.8414e+00,  1.8760e+00,  1.7061e-02, -1.2693e+00, -8.8589e-01,\n",
              "          1.7321e+00,  1.0214e+00, -2.8423e+00, -6.5065e-01,  1.2447e+00,\n",
              "          1.0040e+00,  1.7119e+00,  4.9984e-01,  3.0949e-01, -5.9415e-01,\n",
              "         -4.7672e-01,  1.3525e+00,  9.1809e-01,  9.5314e-02,  7.8741e-01,\n",
              "         -1.2248e+00,  1.8025e+00,  2.5575e-01, -4.5110e-01, -1.0702e-01,\n",
              "          6.5383e-01, -4.1030e-01, -9.6738e-01, -8.8643e-01, -1.8011e+00,\n",
              "         -5.6609e-01,  1.4017e+00, -1.7497e+00, -6.8430e-01, -1.6718e+00,\n",
              "         -3.4879e-01, -1.6657e+00,  6.9776e-01, -3.2934e-01,  1.2025e+00,\n",
              "         -3.7425e-01, -9.6036e-01, -1.2457e-01,  4.2451e-01,  1.6412e-01,\n",
              "         -6.7873e-01, -2.5221e+00, -3.2783e-01, -5.0699e-01, -1.0521e+00,\n",
              "         -2.1176e+00,  1.4534e+00,  1.2327e-03, -6.2381e-01, -9.4255e-01,\n",
              "         -2.5317e-01, -9.3866e-01, -8.3907e-01, -1.3072e+00, -7.5735e-01,\n",
              "          1.6169e+00, -2.4268e-01, -5.0375e-01,  1.2036e+00, -3.8411e-01,\n",
              "         -1.7246e-01,  4.9734e-01, -2.8087e-01, -4.2573e-01, -1.5285e+00,\n",
              "         -1.0469e+00,  2.1370e-01, -1.7906e+00, -2.2417e+00, -2.6809e-01,\n",
              "          2.2873e+00,  2.5796e-01,  8.3874e-01, -1.0480e+00,  1.5925e-01,\n",
              "          1.7272e-01, -1.5338e+00, -1.1793e+00, -6.9750e-01,  1.5132e+00,\n",
              "          1.0381e+00, -4.2169e-01, -4.3489e-01, -6.3125e-01, -2.7449e+00,\n",
              "         -2.2660e+00, -1.3953e+00,  1.8117e+00,  2.2690e-01,  2.6625e-01,\n",
              "         -1.6897e+00, -5.6190e-01, -1.0228e+00,  7.0337e-01,  1.8911e+00,\n",
              "         -4.6141e-01,  3.2687e-01, -5.3925e-01,  3.7898e-01, -4.7561e-01,\n",
              "          2.6070e+00,  1.0229e+00, -2.3119e-01,  1.6142e-01, -7.2506e-01,\n",
              "          1.4663e+00, -4.1458e-01, -1.2553e+00, -1.4919e+00,  6.7669e-01,\n",
              "          4.4470e-01,  7.9358e-01, -1.7087e+00, -8.2017e-01,  8.9666e-02,\n",
              "          2.3374e-01,  7.3127e-01, -1.2606e+00,  1.3242e+00, -6.1114e-01,\n",
              "         -1.7977e+00, -6.1758e-02, -1.1475e+00, -5.5599e-01, -9.7808e-01,\n",
              "          7.2871e-01,  3.1078e-02, -4.5489e-02,  3.6902e-03,  2.7556e-01,\n",
              "         -4.4676e-01,  1.5404e+00,  6.0872e-01, -1.6113e+00, -7.5854e-01,\n",
              "          2.1686e+00, -8.5066e-02, -9.9774e-02, -1.7540e+00,  4.7645e-01,\n",
              "         -8.7628e-01,  2.2558e-01, -1.2949e+00,  3.9069e-01,  2.5905e-01,\n",
              "         -1.6183e+00,  3.1067e-01,  1.5563e+00,  2.9275e-01,  1.0823e+00,\n",
              "         -5.1353e-01,  4.5233e-01,  1.3321e+00,  8.7206e-01,  1.0031e+00,\n",
              "         -2.3585e+00,  8.3533e-01, -2.3624e+00, -1.2225e+00, -7.7529e-01,\n",
              "          5.7090e-01, -8.3794e-01,  1.5205e+00, -3.5906e-01,  8.0456e-01,\n",
              "         -7.6715e-02,  5.6338e-01, -8.5774e-01, -2.0882e-01, -1.4939e+00,\n",
              "         -2.0489e+00,  1.6205e-01,  8.5866e-01, -7.0138e-01,  5.6616e-02,\n",
              "         -7.5278e-01,  2.3950e-01, -1.3783e+00,  5.2783e-01, -8.6258e-01,\n",
              "         -8.0281e-01, -9.4218e-01,  2.0047e+00,  9.2066e-01,  8.2462e-01,\n",
              "         -7.9788e-01,  4.9740e-01, -1.2214e+00, -4.9586e-01, -9.7858e-01,\n",
              "          4.1791e-01,  2.5372e-01, -5.1177e-01, -1.3846e+00, -1.0434e+00,\n",
              "          8.9191e-01,  1.2531e-01], requires_grad=True))"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fc2.weight, model.fc2.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-FJDjuz42mPF",
        "outputId": "03d9f38c-8240-4ca8-cf03-5f240594d720"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Parameter containing:\n",
              " tensor([[42., 42., 42.,  ..., 42., 42., 42.],\n",
              "         [42., 42., 42.,  ..., 42., 42., 42.],\n",
              "         [42., 42., 42.,  ..., 42., 42., 42.],\n",
              "         ...,\n",
              "         [42., 42., 42.,  ..., 42., 42., 42.],\n",
              "         [42., 42., 42.,  ..., 42., 42., 42.],\n",
              "         [42., 42., 42.,  ..., 42., 42., 42.]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([15., 15., 15., 15., 15., 15., 15., 15., 15., 15.], requires_grad=True))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fc3.weight, model.fc3.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3a8suJ9w2npn",
        "outputId": "4c92612c-c59f-4a75-a236-535255e001d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.302585  [   64/60000]\n",
            "loss: 2.270050  [ 6464/60000]\n",
            "loss: 2.217766  [12864/60000]\n",
            "loss: 2.202322  [19264/60000]\n",
            "loss: 2.116242  [25664/60000]\n",
            "loss: 2.096732  [32064/60000]\n",
            "loss: 2.051363  [38464/60000]\n",
            "loss: 1.985575  [44864/60000]\n",
            "loss: 1.942873  [51264/60000]\n",
            "loss: 1.881074  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.1%, Avg loss: 1.851601 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.868556  [   64/60000]\n",
            "loss: 1.842717  [ 6464/60000]\n",
            "loss: 1.696359  [12864/60000]\n",
            "loss: 1.746359  [19264/60000]\n",
            "loss: 1.543325  [25664/60000]\n",
            "loss: 1.560077  [32064/60000]\n",
            "loss: 1.521134  [38464/60000]\n",
            "loss: 1.448230  [44864/60000]\n",
            "loss: 1.442772  [51264/60000]\n",
            "loss: 1.374155  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.5%, Avg loss: 1.339912 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 1.380959  [   64/60000]\n",
            "loss: 1.381377  [ 6464/60000]\n",
            "loss: 1.193579  [12864/60000]\n",
            "loss: 1.340475  [19264/60000]\n",
            "loss: 1.141150  [25664/60000]\n",
            "loss: 1.195134  [32064/60000]\n",
            "loss: 1.188553  [38464/60000]\n",
            "loss: 1.120506  [44864/60000]\n",
            "loss: 1.169126  [51264/60000]\n",
            "loss: 1.110972  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.1%, Avg loss: 1.069310 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 1.109488  [   64/60000]\n",
            "loss: 1.126137  [ 6464/60000]\n",
            "loss: 0.924609  [12864/60000]\n",
            "loss: 1.129630  [19264/60000]\n",
            "loss: 0.963289  [25664/60000]\n",
            "loss: 1.008374  [32064/60000]\n",
            "loss: 1.024469  [38464/60000]\n",
            "loss: 0.948805  [44864/60000]\n",
            "loss: 1.020097  [51264/60000]\n",
            "loss: 0.972273  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 71.4%, Avg loss: 0.926392 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.947563  [   64/60000]\n",
            "loss: 0.984683  [ 6464/60000]\n",
            "loss: 0.770510  [12864/60000]\n",
            "loss: 1.007961  [19264/60000]\n",
            "loss: 0.874691  [25664/60000]\n",
            "loss: 0.894645  [32064/60000]\n",
            "loss: 0.931455  [38464/60000]\n",
            "loss: 0.851454  [44864/60000]\n",
            "loss: 0.927313  [51264/60000]\n",
            "loss: 0.887600  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 72.2%, Avg loss: 0.839966 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0MU8GVBT2sI7",
        "outputId": "563bfabd-8dd4-4989-8abc-1c67d3833276"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[41.9955, 42.0000, 42.0360,  ..., 41.9974, 41.9805, 42.0001],\n",
              "        [41.9862, 42.0000, 42.0260,  ..., 42.0052, 41.9403, 42.0005],\n",
              "        [42.0167, 42.0000, 41.9683,  ..., 42.0037, 41.9771, 42.0001],\n",
              "        ...,\n",
              "        [41.9908, 42.0000, 42.0164,  ..., 42.0000, 42.0509, 42.0089],\n",
              "        [41.9858, 42.0000, 42.0124,  ..., 42.0000, 42.0144, 42.0006],\n",
              "        [41.9936, 42.0000, 41.9179,  ..., 42.0002, 42.1094, 41.9999]],\n",
              "       requires_grad=True)"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.fc3.weight"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "gp_ZPLaS4VX4"
      },
      "source": [
        "# Convolutional Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "SWsPmlUQ4Uw6"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5) \n",
        "    # stride = 1, padding = 0, dilation = 1 by default\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5)\n",
        "    self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "  def forward(self, x):\n",
        "    # input size = 1x28x28\n",
        "    x = self.pool(self.relu(self.conv1(x)))\n",
        "    x = self.pool(self.relu(self.conv2(x)))\n",
        "    x = self.flatten(x)\n",
        "    x = self.relu(self.fc1(x))\n",
        "    x = self.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "net = Net()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "nE6QJ3Lr4qs_"
      },
      "source": [
        "## Redefine the transformation of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "oCJSG2D54hjB"
      },
      "outputs": [],
      "source": [
        "training_data = datasets.FashionMNIST(\n",
        "  root=\"data\",\n",
        "  train=True,\n",
        "  download=True,\n",
        "  transform = ToTensor()\n",
        ")\n",
        "\n",
        "test_data = datasets.FashionMNIST(\n",
        "  root=\"data\",\n",
        "  train=False,\n",
        "  download=True,\n",
        "  transform=ToTensor()\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64)\n",
        "test_dataloader = DataLoader(test_data, batch_size=64)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "bzXSphxu4udS"
      },
      "source": [
        "## Train the CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_xps28Z4dcR",
        "outputId": "4a57f8db-586b-429a-ae69-fc06b40a512f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.300425  [   64/60000]\n",
            "loss: 2.302058  [ 6464/60000]\n",
            "loss: 2.303045  [12864/60000]\n",
            "loss: 2.309356  [19264/60000]\n",
            "loss: 2.283989  [25664/60000]\n",
            "loss: 2.310388  [32064/60000]\n",
            "loss: 2.296953  [38464/60000]\n",
            "loss: 2.303050  [44864/60000]\n",
            "loss: 2.298905  [51264/60000]\n",
            "loss: 2.301833  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 10.5%, Avg loss: 2.298695 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 2.295084  [   64/60000]\n",
            "loss: 2.296698  [ 6464/60000]\n",
            "loss: 2.296507  [12864/60000]\n",
            "loss: 2.304253  [19264/60000]\n",
            "loss: 2.280333  [25664/60000]\n",
            "loss: 2.302902  [32064/60000]\n",
            "loss: 2.292485  [38464/60000]\n",
            "loss: 2.297485  [44864/60000]\n",
            "loss: 2.295645  [51264/60000]\n",
            "loss: 2.295810  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 23.5%, Avg loss: 2.293232 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 2.290836  [   64/60000]\n",
            "loss: 2.291675  [ 6464/60000]\n",
            "loss: 2.290273  [12864/60000]\n",
            "loss: 2.298427  [19264/60000]\n",
            "loss: 2.276127  [25664/60000]\n",
            "loss: 2.293890  [32064/60000]\n",
            "loss: 2.285805  [38464/60000]\n",
            "loss: 2.289422  [44864/60000]\n",
            "loss: 2.291024  [51264/60000]\n",
            "loss: 2.286370  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 19.4%, Avg loss: 2.284866 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 2.284409  [   64/60000]\n",
            "loss: 2.284193  [ 6464/60000]\n",
            "loss: 2.280975  [12864/60000]\n",
            "loss: 2.289505  [19264/60000]\n",
            "loss: 2.267432  [25664/60000]\n",
            "loss: 2.279387  [32064/60000]\n",
            "loss: 2.273485  [38464/60000]\n",
            "loss: 2.274168  [44864/60000]\n",
            "loss: 2.281816  [51264/60000]\n",
            "loss: 2.268470  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 18.2%, Avg loss: 2.267938 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 2.270341  [   64/60000]\n",
            "loss: 2.268006  [ 6464/60000]\n",
            "loss: 2.260606  [12864/60000]\n",
            "loss: 2.270837  [19264/60000]\n",
            "loss: 2.245232  [25664/60000]\n",
            "loss: 2.244549  [32064/60000]\n",
            "loss: 2.243036  [38464/60000]\n",
            "loss: 2.235642  [44864/60000]\n",
            "loss: 2.253482  [51264/60000]\n",
            "loss: 2.221164  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 25.2%, Avg loss: 2.221458 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 2.230397  [   64/60000]\n",
            "loss: 2.220715  [ 6464/60000]\n",
            "loss: 2.199551  [12864/60000]\n",
            "loss: 2.210028  [19264/60000]\n",
            "loss: 2.174166  [25664/60000]\n",
            "loss: 2.131025  [32064/60000]\n",
            "loss: 2.130568  [38464/60000]\n",
            "loss: 2.092192  [44864/60000]\n",
            "loss: 2.110901  [51264/60000]\n",
            "loss: 2.004427  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 33.1%, Avg loss: 2.014256 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 2.041865  [   64/60000]\n",
            "loss: 1.983562  [ 6464/60000]\n",
            "loss: 1.889830  [12864/60000]\n",
            "loss: 1.873682  [19264/60000]\n",
            "loss: 1.732140  [25664/60000]\n",
            "loss: 1.627775  [32064/60000]\n",
            "loss: 1.567609  [38464/60000]\n",
            "loss: 1.500558  [44864/60000]\n",
            "loss: 1.500062  [51264/60000]\n",
            "loss: 1.339259  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 57.2%, Avg loss: 1.349707 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 1.412947  [   64/60000]\n",
            "loss: 1.346116  [ 6464/60000]\n",
            "loss: 1.168032  [12864/60000]\n",
            "loss: 1.214245  [19264/60000]\n",
            "loss: 1.074459  [25664/60000]\n",
            "loss: 1.089459  [32064/60000]\n",
            "loss: 1.088909  [38464/60000]\n",
            "loss: 1.066462  [44864/60000]\n",
            "loss: 1.091886  [51264/60000]\n",
            "loss: 1.043767  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.2%, Avg loss: 1.026794 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 1.060518  [   64/60000]\n",
            "loss: 1.097193  [ 6464/60000]\n",
            "loss: 0.882977  [12864/60000]\n",
            "loss: 1.026770  [19264/60000]\n",
            "loss: 0.900573  [25664/60000]\n",
            "loss: 0.938241  [32064/60000]\n",
            "loss: 0.953550  [38464/60000]\n",
            "loss: 0.954943  [44864/60000]\n",
            "loss: 0.956666  [51264/60000]\n",
            "loss: 0.998572  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 64.0%, Avg loss: 0.945213 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 0.933581  [   64/60000]\n",
            "loss: 1.041026  [ 6464/60000]\n",
            "loss: 0.790538  [12864/60000]\n",
            "loss: 0.982161  [19264/60000]\n",
            "loss: 0.864704  [25664/60000]\n",
            "loss: 0.889666  [32064/60000]\n",
            "loss: 0.902392  [38464/60000]\n",
            "loss: 0.913242  [44864/60000]\n",
            "loss: 0.889159  [51264/60000]\n",
            "loss: 0.982194  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 65.6%, Avg loss: 0.905138 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 10\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, net, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, net, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "t-0YPCR48rmD"
      },
      "source": [
        "## Setting custom kernels\n",
        "\n",
        "Just like with model weights, we can set specific kernels for CNNs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "KqAPP8BZ4twp"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Net, self).__init__()\n",
        "    # how many dimensions does the bias vector of self.conv1 have?\n",
        "    self.conv1 = nn.Conv2d(in_channels=1, out_channels=6, kernel_size=5, stride=1) \n",
        "    # stride = 1, padding = 0, dilation = 1 by default\n",
        "    self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "    self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
        "    self.fc1 = nn.Linear(16 * 4 * 4, 120)\n",
        "    self.fc2 = nn.Linear(120, 84)\n",
        "    self.fc3 = nn.Linear(84, 10)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.flatten = nn.Flatten()\n",
        "\n",
        "    with torch.no_grad():\n",
        "      weights = torch.FloatTensor([\n",
        "          [0, 1, 0, 1, 0],\n",
        "          [1, 0, 1, 0, 1],\n",
        "          [1, 0, 1, 0, 1],\n",
        "          [0, 0, 1, 0, 1],\n",
        "          [1, 1, 1, 1, 0]\n",
        "      ])\n",
        "      weights = weights.view(1, 1, 5, 5).repeat(6, 1, 1, 1)\n",
        "      # here we have 6 copies of the same kernel\n",
        "      # as you can imagine, you must initialise each kernel\n",
        "      # as you please. It doesn't have to be the same kernel for\n",
        "      # each channel\n",
        "\n",
        "      # why do we need 6 kernels for conv1?\n",
        "      self.conv1.weight = nn.Parameter(weights, requires_grad=True)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # input size = 1x28x28\n",
        "    x = self.pool(self.relu(self.conv1(x)))\n",
        "    x = self.pool(self.relu(self.conv2(x)))\n",
        "    x = self.flatten(x)\n",
        "    x = self.relu(self.fc1(x))\n",
        "    x = self.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return x\n",
        "\n",
        "net = Net()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0rozmu89Xxb",
        "outputId": "d889bde1-0232-4ef8-8fef-f3c70459c75e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(Parameter containing:\n",
              " tensor([[[[0., 1., 0., 1., 0.],\n",
              "           [1., 0., 1., 0., 1.],\n",
              "           [1., 0., 1., 0., 1.],\n",
              "           [0., 0., 1., 0., 1.],\n",
              "           [1., 1., 1., 1., 0.]]],\n",
              " \n",
              " \n",
              "         [[[0., 1., 0., 1., 0.],\n",
              "           [1., 0., 1., 0., 1.],\n",
              "           [1., 0., 1., 0., 1.],\n",
              "           [0., 0., 1., 0., 1.],\n",
              "           [1., 1., 1., 1., 0.]]],\n",
              " \n",
              " \n",
              "         [[[0., 1., 0., 1., 0.],\n",
              "           [1., 0., 1., 0., 1.],\n",
              "           [1., 0., 1., 0., 1.],\n",
              "           [0., 0., 1., 0., 1.],\n",
              "           [1., 1., 1., 1., 0.]]],\n",
              " \n",
              " \n",
              "         [[[0., 1., 0., 1., 0.],\n",
              "           [1., 0., 1., 0., 1.],\n",
              "           [1., 0., 1., 0., 1.],\n",
              "           [0., 0., 1., 0., 1.],\n",
              "           [1., 1., 1., 1., 0.]]],\n",
              " \n",
              " \n",
              "         [[[0., 1., 0., 1., 0.],\n",
              "           [1., 0., 1., 0., 1.],\n",
              "           [1., 0., 1., 0., 1.],\n",
              "           [0., 0., 1., 0., 1.],\n",
              "           [1., 1., 1., 1., 0.]]],\n",
              " \n",
              " \n",
              "         [[[0., 1., 0., 1., 0.],\n",
              "           [1., 0., 1., 0., 1.],\n",
              "           [1., 0., 1., 0., 1.],\n",
              "           [0., 0., 1., 0., 1.],\n",
              "           [1., 1., 1., 1., 0.]]]], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([ 0.0301,  0.1273, -0.0666, -0.0118,  0.0198, -0.1475],\n",
              "        requires_grad=True))"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net.conv1.weight, net.conv1.bias"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFd1P9je964G",
        "outputId": "212c045c-a680-465e-abab-d6d0a4fc6966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.269511  [   64/60000]\n",
            "loss: 2.114079  [ 6464/60000]\n",
            "loss: 1.767052  [12864/60000]\n",
            "loss: 1.519192  [19264/60000]\n",
            "loss: 1.261594  [25664/60000]\n",
            "loss: 1.148371  [32064/60000]\n",
            "loss: 1.074535  [38464/60000]\n",
            "loss: 0.979699  [44864/60000]\n",
            "loss: 1.025754  [51264/60000]\n",
            "loss: 0.976575  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 63.8%, Avg loss: 0.961789 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 0.958201  [   64/60000]\n",
            "loss: 1.026418  [ 6464/60000]\n",
            "loss: 0.794304  [12864/60000]\n",
            "loss: 0.971024  [19264/60000]\n",
            "loss: 0.876761  [25664/60000]\n",
            "loss: 0.881821  [32064/60000]\n",
            "loss: 0.875509  [38464/60000]\n",
            "loss: 0.823083  [44864/60000]\n",
            "loss: 0.844957  [51264/60000]\n",
            "loss: 0.910138  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.5%, Avg loss: 0.859914 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.805608  [   64/60000]\n",
            "loss: 0.939797  [ 6464/60000]\n",
            "loss: 0.666275  [12864/60000]\n",
            "loss: 0.891263  [19264/60000]\n",
            "loss: 0.839616  [25664/60000]\n",
            "loss: 0.797984  [32064/60000]\n",
            "loss: 0.798583  [38464/60000]\n",
            "loss: 0.765138  [44864/60000]\n",
            "loss: 0.769871  [51264/60000]\n",
            "loss: 0.865053  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 69.1%, Avg loss: 0.806713 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.740910  [   64/60000]\n",
            "loss: 0.865193  [ 6464/60000]\n",
            "loss: 0.592725  [12864/60000]\n",
            "loss: 0.839247  [19264/60000]\n",
            "loss: 0.803082  [25664/60000]\n",
            "loss: 0.760038  [32064/60000]\n",
            "loss: 0.749208  [38464/60000]\n",
            "loss: 0.717325  [44864/60000]\n",
            "loss: 0.727149  [51264/60000]\n",
            "loss: 0.831633  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.7%, Avg loss: 0.767815 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.697248  [   64/60000]\n",
            "loss: 0.804714  [ 6464/60000]\n",
            "loss: 0.549599  [12864/60000]\n",
            "loss: 0.794849  [19264/60000]\n",
            "loss: 0.768176  [25664/60000]\n",
            "loss: 0.729994  [32064/60000]\n",
            "loss: 0.718542  [38464/60000]\n",
            "loss: 0.679046  [44864/60000]\n",
            "loss: 0.693904  [51264/60000]\n",
            "loss: 0.794751  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 72.5%, Avg loss: 0.736004 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, net, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, net, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aUuziRqoa7B",
        "outputId": "42ed2812-f09b-4ef6-bbf2-0e119e7d5308"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[0.0080, 1.0123, 0.0096, 1.0089, 0.0064],\n",
              "          [1.0093, 0.0117, 1.0085, 0.0069, 1.0052],\n",
              "          [1.0089, 0.0104, 1.0074, 0.0053, 1.0039],\n",
              "          [0.0086, 0.0102, 1.0072, 0.0047, 1.0035],\n",
              "          [1.0084, 1.0096, 1.0061, 1.0036, 0.0028]]],\n",
              "\n",
              "\n",
              "        [[[0.0057, 1.0073, 0.0047, 1.0062, 0.0047],\n",
              "          [1.0068, 0.0078, 1.0057, 0.0062, 1.0044],\n",
              "          [1.0064, 0.0073, 1.0050, 0.0053, 1.0031],\n",
              "          [0.0055, 0.0071, 1.0052, 0.0051, 1.0029],\n",
              "          [1.0055, 1.0073, 1.0053, 1.0053, 0.0027]]],\n",
              "\n",
              "\n",
              "        [[[0.0088, 1.0132, 0.0111, 1.0125, 0.0094],\n",
              "          [1.0105, 0.0135, 1.0112, 0.0123, 1.0094],\n",
              "          [1.0107, 0.0133, 1.0104, 0.0113, 1.0082],\n",
              "          [0.0104, 0.0135, 1.0102, 0.0105, 1.0075],\n",
              "          [1.0111, 1.0133, 1.0091, 1.0093, 0.0070]]],\n",
              "\n",
              "\n",
              "        [[[0.0094, 1.0123, 0.0096, 1.0083, 0.0031],\n",
              "          [1.0107, 0.0119, 1.0096, 0.0079, 1.0037],\n",
              "          [1.0097, 0.0109, 1.0089, 0.0070, 1.0032],\n",
              "          [0.0086, 0.0108, 1.0088, 0.0069, 1.0036],\n",
              "          [1.0085, 1.0109, 1.0082, 1.0066, 0.0038]]],\n",
              "\n",
              "\n",
              "        [[[0.0055, 1.0073, 0.0068, 1.0078, 0.0048],\n",
              "          [1.0062, 0.0068, 1.0068, 0.0075, 1.0045],\n",
              "          [1.0056, 0.0061, 1.0065, 0.0073, 1.0038],\n",
              "          [0.0051, 0.0059, 1.0063, 0.0070, 1.0036],\n",
              "          [1.0045, 1.0052, 1.0057, 1.0063, 0.0033]]],\n",
              "\n",
              "\n",
              "        [[[0.0068, 1.0090, 0.0074, 1.0072, 0.0041],\n",
              "          [1.0068, 0.0081, 1.0061, 0.0049, 1.0027],\n",
              "          [1.0070, 0.0081, 1.0057, 0.0039, 1.0023],\n",
              "          [0.0076, 0.0086, 1.0056, 0.0035, 1.0022],\n",
              "          [1.0075, 1.0085, 1.0050, 1.0027, 0.0020]]]], requires_grad=True)"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net.conv1.weight"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ik1v8fJYreOD"
      },
      "source": [
        "### Freezing a specific kernel for training\n",
        "\n",
        "Similarly to a dense neural network, we can freeze the kernels and train. Let's see what happens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c711q5OZrXfA",
        "outputId": "1d6be18b-f257-4745-b7d3-fc5132d134fe"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[0., 1., 0., 1., 0.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [0., 0., 1., 0., 1.],\n",
              "          [1., 1., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 1., 0., 1., 0.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [0., 0., 1., 0., 1.],\n",
              "          [1., 1., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 1., 0., 1., 0.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [0., 0., 1., 0., 1.],\n",
              "          [1., 1., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 1., 0., 1., 0.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [0., 0., 1., 0., 1.],\n",
              "          [1., 1., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 1., 0., 1., 0.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [0., 0., 1., 0., 1.],\n",
              "          [1., 1., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 1., 0., 1., 0.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [0., 0., 1., 0., 1.],\n",
              "          [1., 1., 1., 1., 0.]]]])"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net = Net()\n",
        "net.conv1.weight.requires_grad_(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPC2-CUzr5Zg",
        "outputId": "15989760-3082-4250-c3fc-06446e76fbb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 2.332624  [   64/60000]\n",
            "loss: 2.237638  [ 6464/60000]\n",
            "loss: 2.086502  [12864/60000]\n",
            "loss: 1.941406  [19264/60000]\n",
            "loss: 1.584647  [25664/60000]\n",
            "loss: 1.419474  [32064/60000]\n",
            "loss: 1.275614  [38464/60000]\n",
            "loss: 1.137595  [44864/60000]\n",
            "loss: 1.144067  [51264/60000]\n",
            "loss: 1.013160  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 62.8%, Avg loss: 1.010004 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 1.058013  [   64/60000]\n",
            "loss: 1.042434  [ 6464/60000]\n",
            "loss: 0.810853  [12864/60000]\n",
            "loss: 0.986306  [19264/60000]\n",
            "loss: 0.898038  [25664/60000]\n",
            "loss: 0.935231  [32064/60000]\n",
            "loss: 0.900562  [38464/60000]\n",
            "loss: 0.837594  [44864/60000]\n",
            "loss: 0.875587  [51264/60000]\n",
            "loss: 0.868203  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 67.3%, Avg loss: 0.859339 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 0.867578  [   64/60000]\n",
            "loss: 0.896618  [ 6464/60000]\n",
            "loss: 0.670783  [12864/60000]\n",
            "loss: 0.892644  [19264/60000]\n",
            "loss: 0.847780  [25664/60000]\n",
            "loss: 0.851791  [32064/60000]\n",
            "loss: 0.820043  [38464/60000]\n",
            "loss: 0.751066  [44864/60000]\n",
            "loss: 0.778591  [51264/60000]\n",
            "loss: 0.802853  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 70.1%, Avg loss: 0.792085 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 0.786315  [   64/60000]\n",
            "loss: 0.804941  [ 6464/60000]\n",
            "loss: 0.598984  [12864/60000]\n",
            "loss: 0.838299  [19264/60000]\n",
            "loss: 0.798527  [25664/60000]\n",
            "loss: 0.806041  [32064/60000]\n",
            "loss: 0.776122  [38464/60000]\n",
            "loss: 0.681418  [44864/60000]\n",
            "loss: 0.724016  [51264/60000]\n",
            "loss: 0.751755  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 72.1%, Avg loss: 0.750031 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 0.736820  [   64/60000]\n",
            "loss: 0.737639  [ 6464/60000]\n",
            "loss: 0.555436  [12864/60000]\n",
            "loss: 0.808166  [19264/60000]\n",
            "loss: 0.753557  [25664/60000]\n",
            "loss: 0.774668  [32064/60000]\n",
            "loss: 0.747036  [38464/60000]\n",
            "loss: 0.636167  [44864/60000]\n",
            "loss: 0.682130  [51264/60000]\n",
            "loss: 0.706697  [57664/60000]\n",
            "Test Error: \n",
            " Accuracy: 73.6%, Avg loss: 0.720527 \n",
            "\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate)\n",
        "\n",
        "epochs = 5\n",
        "for t in range(epochs):\n",
        "  print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "  train_loop(train_dataloader, net, loss_fn, optimizer)\n",
        "  test_loop(test_dataloader, net, loss_fn)\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QG9mQcxr_SB",
        "outputId": "57a0ff0c-3aff-4289-9617-d5c36c72bcd4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Parameter containing:\n",
              "tensor([[[[0., 1., 0., 1., 0.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [0., 0., 1., 0., 1.],\n",
              "          [1., 1., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 1., 0., 1., 0.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [0., 0., 1., 0., 1.],\n",
              "          [1., 1., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 1., 0., 1., 0.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [0., 0., 1., 0., 1.],\n",
              "          [1., 1., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 1., 0., 1., 0.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [0., 0., 1., 0., 1.],\n",
              "          [1., 1., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 1., 0., 1., 0.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [0., 0., 1., 0., 1.],\n",
              "          [1., 1., 1., 1., 0.]]],\n",
              "\n",
              "\n",
              "        [[[0., 1., 0., 1., 0.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [1., 0., 1., 0., 1.],\n",
              "          [0., 0., 1., 0., 1.],\n",
              "          [1., 1., 1., 1., 0.]]]])"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "net.conv1.weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz6ubYIVuUdw"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
