{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import os\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks Laboratory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A first, shallow Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To check if the CUDA version of Pytorch was installed successfully, only if your machine supports CUDA, you can do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input of a neural network will be composed of two tensors: the sample and the label.\n",
    "\n",
    "Imagine we have a classification task with 3 possible labels (0,1,2) and samples of 5 features (5-dimensional samples).\n",
    "\n",
    "We will have tensors like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1,5)\n",
    "y = torch.randint(3, (1,)).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4045, 0.0240, 0.9723, 0.0044, 0.8372]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us define a shallow, very simple Neural Network.\n",
    "\n",
    "The Neural Network will have: \n",
    "1. As many input neurons as sample features (5 in this case)\n",
    "2. An hidden layer \n",
    "3. As many ouput neurons as possible labels (3 in this case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images/shallow_nn.png](images/shallow_nn.png) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This schema entails 4 different operations:\n",
    "1. Matrix multiplication with the weights of the connections from the input layer to the FC Layer\n",
    "2. Activation function in the FC Layer\n",
    "3. Matrix multiplication with the weights of the connections from the FC Layer to the output layer\n",
    "4. Activation function in the output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_hidden, output_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.fc1 = nn.Linear(input_size, num_hidden)\n",
    "        self.output_layer = nn.Linear(num_hidden, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step by step computation of a forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we start with the tensor x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4045, 0.0240, 0.9723, 0.0044, 0.8372]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x goes through the hidden layer. What this means is that we obtain the output:\n",
    "\n",
    "$ Wx + b $ where:\n",
    "1. W is the weight matrix of the connections\n",
    "2. b is the bias vector\n",
    "\n",
    "These values depend on the chosen initialization and might be random at first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.0070,  0.1379, -0.2972, -0.3857,  0.3767],\n",
      "        [ 0.3181,  0.3841, -0.4351, -0.0183, -0.0194],\n",
      "        [-0.2397, -0.0694,  0.4098,  0.4147,  0.2735],\n",
      "        [-0.1449, -0.1749,  0.3753,  0.3086,  0.2791],\n",
      "        [-0.0386,  0.0840, -0.4012, -0.1547,  0.2860],\n",
      "        [-0.1574, -0.1003,  0.0241, -0.0021, -0.3871],\n",
      "        [-0.2208,  0.1001,  0.0337,  0.2741, -0.2626],\n",
      "        [-0.1746, -0.3350, -0.3427, -0.0808,  0.1582],\n",
      "        [ 0.3688, -0.2545,  0.0900,  0.3000, -0.2573],\n",
      "        [-0.0169, -0.4140,  0.3259,  0.1208,  0.3073]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-0.2639,  0.0529,  0.3420, -0.4028,  0.2408,  0.2938, -0.1459,  0.2246,\n",
      "        -0.0782, -0.3392], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "hidden_layer = nn.Linear(5,10)\n",
    "\n",
    "print( hidden_layer.weight)\n",
    "\n",
    "print( hidden_layer.bias )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2387, -0.2485,  0.8727,  0.1342,  0.0759, -0.0730, -0.4187, -0.0552,\n",
       "         -0.0617,  0.2186]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = hidden_layer.weight\n",
    "b = hidden_layer.bias\n",
    "\n",
    "output = torch.matmul(x, w.t()) + b\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we apply the hidden layer, we obtain the same results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2387, -0.2485,  0.8727,  0.1342,  0.0759, -0.0730, -0.4187, -0.0552,\n",
       "         -0.0617,  0.2186]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden_layer(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images/dnn_2.jpeg](images/dnn_2.jpeg) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to introduce non-linearity into our net, allowing it to learn more complex patterns, we pass the output of the FC layer through the ReLU activation function, which looks like this:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images/relu.png](images/relu.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2387, -0.2485,  0.8727,  0.1342,  0.0759, -0.0730, -0.4187, -0.0552,\n",
       "         -0.0617,  0.2186]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.8727, 0.1342, 0.0759, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.2186]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu = nn.ReLU()\n",
    "\n",
    "output = relu(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000, 0.0000, 0.8727, 0.1342, 0.0759, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.2186]], grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relu(hidden_layer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we go through another round of linear multiplication from the nodes inside the hidden layer to the nodes in the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1568,  0.2774, -0.3305]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer = nn.Linear(10,3)\n",
    "\n",
    "weights_2 = output_layer.weight \n",
    "bias_2 = output_layer.bias\n",
    "\n",
    "output = torch.matmul(output, weights_2.t()) + bias_2\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1568,  0.2774, -0.3305]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_layer(relu(hidden_layer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have obtained a tensor of the same size of the output layer, which is the same size of the number of possible labels in our classification task.\n",
    "\n",
    "\n",
    "In order to convert these values to probabilities, we apply the SoftMax activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![images/softmax.png](images/softmax.png) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3646, 0.4114, 0.2240]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax = nn.Softmax(dim=1)\n",
    "\n",
    "output = softmax(output)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3646, 0.4114, 0.2240]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax(output_layer(relu(hidden_layer(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outputs are the probabilities of the sample being labeled with each one of the labels (0,1,2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We manually implemented a forward pass of the network we defined at the start:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowNeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, num_hidden, output_size):\n",
    "        super().__init__()\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.fc1 = nn.Linear(input_size, num_hidden)\n",
    "        self.output_layer = nn.Linear(num_hidden, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.output_layer(x)\n",
    "        x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "shallow_nn = ShallowNeuralNetwork(5,10,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The values might be slightly different due to the randomization of the starting weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2929, 0.3337, 0.3734]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shallow_nn(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
